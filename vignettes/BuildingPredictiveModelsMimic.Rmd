---
title: "Building Patient-Level Predictive Model with MIMIC-III"
author: "Eduardo Angulo, Giovanny Barbosa, Carlos Conrado, Cristhyan De Marchena, Emmanuel Gutiérrez, Kamila Hernandez, Deivis Martínez, Omar Mejia"
date: '`r Sys.Date()`' #"26/5/2022"
output: html_document
---

# Introduction

In health care, the clinical characterization of the progress of diseases, the analysis of the effect and behavior of medication consumption and the prediction of patient outcome is a fundamental step to save lives, improve the efficiency in the provision of the service and failing that, reduce the costs incurred. Therefore, being able to accurately forecast what will happen to patients, at scale, is key to improving the health of the population and of individual patients.

The prediction problem addressed in this vignette is associated with the population at risk. The main goal is to predict which patients at a defined time (t = 0), will die at some previously established time at risk. The prediction is made using only information from patients belonging to an observation window of at least 365 days prior to the time of risk. In order to have a better representation of the prediction problem, we can abroad it as follows:

<center>

![Observation period](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654050927/MIMIC-III/observation_period_rxjloz.png)

</center>


As shown in the figure below, to define a prediction problem, the first thing that must be defined is an initial time from which the data will be taken in order to generate the target cohort (T), additionally, we must be define the outcome we would like to predict in the form of an outcome cohort(O) and also we must consider the time at risk (TAR). Then, the pacient entries must be crossed between the target cohort (T) and the outcome cohort (O), thus we can obtain the input information for a given model and the prediction technique we will be using. It's important to note that those cases belonging to the outcome cohort (O) which have occurred during the observation period must be excluded from the model, since this scenario can induce not getting patients at the outcome cohort (O) at all.


<center>

![Prediction problem](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654050926/MIMIC-III/prediction_problem_xqiirw.png)

</center>

<center>

![Target outcome](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654050927/MIMIC-III/target_outcome_xykdtj.png)

</center>


It's very important for us as researchers, to develop the ability of benefitting from large databases, such as clinical patient records (also known as EHR, or Electronic Health Record), signals, laboratory tests, referrals to hospitals, among others. Thus, through analysis and machine learning techniques, driven by a massive increase in computational power, unprecedented opportunities are bound to open so we can improve patient healthcare in order to save as many lives as possible.

Consequently, it's key to work on the standardization process of developing, creating and presenting prediction models, so that researchers may focus on the results and the analysis of several models instead of programming it. Therefore, we decided to create this vignette, which gives a practical guide on how to create prediction models based on the information stored in the [MIMIC-III Clinical Database](https://physionet.org/content/mimiciii/1.4/), using powerful tools such as [ATLAS](https://atlas.ohdsi.org/#/home) that saves us a lot of time regarding information extraction in the form of cohorts.

Thus, this work describes how the [PatientLevelPrediction package](https://ohdsi.github.io/PatientLevelPrediction/) can be used for building patient-level predictive models. It's worth noticing that this package provides powerful tools that allow data extraction, covariates generation for building a model and also allows us to test it using data from [MIMIC-III Clinical Database](https://physionet.org/content/mimiciii/1.4/) which currently isn't only the largest EHR database but also is the most suitable for these kind of predictive models.

# Running configuration

In this section, we present a form of executing this vignette by configuring a Docker container in your local or virtual Machine (VM) in the cloud. We assume that you have access, locally or in the cloud, to the [MIMIC-III Clinical Database](https://physionet.org/content/mimiciii/1.4/) in the [OMOP Common Data Model (CDM)](https://ohdsi.github.io/TheBookOfOhdsi/CommonDataModel.html) format, thus we proceed as follows:
- If you are using a VM in the cloud you need to connect to it via SSH and establish a SSH tunnel using `ssh -L <local_port>:localhost:8787 <vm_user>@<vm_ip>`.
- Install [Docker Engine](https://docs.docker.com/engine/install/) if not installed.
- Clone the [OHDSI/PatientLevelPrediction](https://github.com/OHDSI/PatientLevelPrediction) repository.
- Make sure that the port 8787 is available on the machine that will run the Docker container.
- Run the following command `docker run -d --name=<docker_name> --network=host -v <path_to_repository>:/home/ohdsi/workdir -e USER=ohdsi -e PASSWORD=ohdsi odysseusinc/rstudio-ohdsi:latest`. If the docker user is not on sudoers list/group, you may need to run it as sudo.
- On your browser, access to `http://localhost:8787` and login to rstudio using the following credentials:
  - **Username:** ohdsi
  - **Password:** ohdsi
- We are using `PatientLevelPrediction` version 5.0.5, you can check the version of `PatientLevelPrediction` package by running `packageVersion('PatientLevelPrediction')` on the rstudio console. If you have a version mismatch run the following commands:
  ```
    install.packages('remotes')
    remotes::install_github('OHDSI/FeatureExtraction')
    remotes::install_github('OHDSI/PatientLevelPrediction')
  ```
  - If a confirmation is needed, press enter.
  - Restart the rstudio session.
- On the file explorer, search for `vignettes/BuildingPredictiveModelsMimic.Rmd` file and run it.

# Study specification

To perform this study is necessary to define the prediction problem that will be addressed, the target and outcome cohorts,  which algorithm will be implemented for the model, and how the model will be evaluated and validated. Next, a case of a patient-level predictive model is presented.

## Problem Definition

Cardiovascular Disease (CVD) is a term used to group several conditions that affect the heart or blood vessels. These are the leading cause of death globally. According to World Health Organization, about 17.9 million people died due to CVDs in 2019, which represents 32% of all global deaths. On the other hand, heart disease, a type of CVD, is the cause of about 659,000 deads in the United States, that is, one in every four. By the above, the need to develop predictive models that allow detecting the risk of the patient is observed.

In this work, the PatientLevelPrediction package is applied to observational healthcare data to address the following patient-level prediction question:

Which heart disease patients admitted into the intensive care unit (ICU) have a high risk of death?

The target cohort was defined as the patients who have been diagnosed with heart disease anytime and were admitted into ICU before the index time. On the other hand, the outcome cohort was defined as the patients that passed away.

## Study population definition

To define the population of this study is necessary to keep in mind the following:
- The observation time used in this study was defined as any time before the index time.
- In this study was defined that patients do not enter multiple times in the target cohort even if they have several diagnoses that qualified to get into it.
- Keeping in mind that the outcome of this study is death, it is not allowed the entry of patients that had experienced the outcome without getting into the target cohort.
- In this study, the predictions will be performed in a ‘time-at-risk’ window starting one day after the start of the target cohort up to 90 days later.
- A minimum time-at-risk of 30 days was defined for this study. The above was performed to include patients that do not experience the outcome earlier than the end of the time-at-risk period and exit the database.

## Model development settings
Due to the nature of our problem we have to abide to a binary classification, thus we selected two target models:

| **Algorithm**                   | **Description**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | **Hyper-parameters**                                                                                                                                                |
|---------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Regularized Logistic Regression | Lasso logistic regression belongs to the family of generalized linear models, where a linear combination of the variables is learned and finally a logistic function maps the linear combination to a value between 0 and 1. The lasso regularization adds a cost based on model complexity to the objective function when training the model. This cost is the sum of the absolute values of the linear combination of the coefficients. The model automatically performs feature selection by minimizing this cost. | `var` (starting variance), `seed`                                                                                                                                   |
| Gradient boosting machines      | Gradient boosting machines is a boosting ensemble technique and in our framework it combines multiple decision trees. Boosting works by iteratively adding decision trees but adds more weight to the data-points that are misclassified by prior decision trees in the cost function when training the next tree.                                                                                                                                                                                                    | `ntree` (number of trees), `max depth` (max levels in tree), `min rows` (minimum data points in in node), `learning rate`, `balance` (balance class labels), `seed` |

Furthermore, we have to decide on the **covariates** that we will use to train our model. In our example, we add covariates such as:
- Age
- Age Group
- Chads2
- Chads2 Vasc
- Charlson Index
- Condition Era
  - Any Time Prior
- Condition Group Era
  - Long Term
  - Any Time Prior
- Condition Occurrence
  - Short Term
  - Medium Term
  - Long Term
  - Any Time Prior
- Dcsi
- Drug Group Era
  - Long Term
  - Any Time Prior
- Gender
- Measurement
  - Any Time Prior
- Measurement Value
  - Any Time Prior
- Procedure Occurrence
  - Any Time Prior
- Race
- Visit Concept Count
  - Long Term

## Study implementation
Now we have completely designed our study, and we have to implement it. We have to generate the target and outcome cohorts and we need to develop the R code to run against our MIMIC-III database that will execute the analysis.

### Cohort instantiation
In order to load our cohorts, we leverage some of the work by generating the SQL script with [ATLAS](#atlas-cohort-builder). The result table would have the following structure:

| **Column**             | **Description**                                                                                             |
|------------------------|-------------------------------------------------------------------------------------------------------------|
| `cohort_definition_id` | A unique identifier for distinguishing between different types of cohorts, e.g. target and outcome cohorts. |
| `subject_id`           | A unique identifier corresponding to the person_id in the CDM.                                              |
| `cohort_start_date`    | The date the subject enters the cohort.                                                                     |
| `cohort_end_date`      | The date the subject leaves the cohort.                                                                     |

### ATLAS cohort builder

Firstly, go to [ATLAS atlas.ohdsi.org](https://atlas.ohdsi.org/#/home) in production, where you are asked to be logged in, select "Sign in" at the top right or log in at the center of the page.

![Figure 1: production environment](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033085/MIMIC-III/Atlas_1_p0lo6c.png)

Then, register with a valid Google account to be able to continue on the platform.

![Figure 2: production environment - Register User](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033085/MIMIC-III/Atlas_2_gxu67h.png)

Once inside ATLAS, it is verified that we are in the **Cohort Definitions** section in the menu on the left.

![Figure 3: production environment - Cohort Definitions](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_3_ul5lgl.png)

In the **filter text box** at the top right, filter by the name of the cohort or a keyword that allows us to find the cohort that is required. In this study we do it with the word **heart**. As shown in the figure below.

![Figure 4: production environment - Filter](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_3_2_jpxkx9.png)

**Prevalent heart disease** is selected from the filtered options for this investigation.

![Figure 5: production environment - Select Cohort](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_4_2_nocjr0.png)

After selecting the filters, when the **Definition** tab  appears, we go to the **Export** tab and within it select **JSON** and we mark **Copy to clipboard** to have it in memory as shown in the following image:

![Figure 6: production environment - Copy JSON](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_6_2_ftfyaa.png)
Once the data is copied into memory as a template, it is passed to [ATLAS DEMO atlas-demo.ohdsi.org](https://atlas-demo.ohdsi.org/#/home) where its own cohort is created based on the template data that we brought.

In this platform it is not necessary to be registered, so we directly select **Cohort Definitions**.

![Figure 7: demo environment - Cohort Definitions (New Cohort)](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_7_2_wiwgzk.png)

In this demo section we can create our own cohort by selecting **New Cohort**

![Figure 8: demo environment - New Cohort](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_8_2_h4ttys.png)

At this point we go to **Export** within this **JSON** tab and paste what was copied into the production environment to serve as a template and mark **Reload**.

![Figure 9: demo environment - Reload](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_9_2_dzhsvq.png)

With the template already copied, we proceed to name it as considered according to the study that is being carried out and we add the initial events and the attributes of the cohort that is being configured for its subsequent application.

![Figure 10: demo environment - Save new cohort](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033084/MIMIC-III/Atlas_demo_10_2_bxhob0.png)

After configuring the previous characteristics and properly naming the new cohort, it is saved in the disk button on the right side of the text box where the name was written.

Once the above is done, we go to the **Export** tab, then **SQL**, then **Template OHDSI.SQL** and copy the SQL content to take it to our **RStudio** and paste it into a file with .sql that will be taken into our model application.

![Figure 11: demo environment - Copy SQL](https://res.cloudinary.com/uninorte-bioinformatics/image/upload/v1654033085/MIMIC-III/Atlas_demo_11_2_psxxaz.png)

In the present study, the [Target cohort](https://atlas-demo.ohdsi.org/#/cohortdefinition/1779448) were defined as described in the previous images and the [Outcome cohort](https://atlas-demo.ohdsi.org/#/cohortdefinition/1770232) found in the DEMO environment at these respective links.

It should be noted that for the **Outcome Cohort**, the procedure is the same as indicated in the previous steps as in the **Target Cohort**.

### Database connection
After we have got the SQL script from ATLAS, it's worth noticing that prior to stablishing a connection with our database we need have a JDBC driver, we can download the driver for postgres by running the following R code:
```{r echo=F, results='hide'}
base_path = '/home/ohdsi'
driver_path = paste0(base_path, '/drivers')

DatabaseConnector::downloadJdbcDrivers(
  	dbms = 'postgresql',
  	pathToDriver = driver_path
)
```

Due to downloading the driver, among other reasons, we move our working directory and set some environment variables:
```{r echo=F, results='hide'}
Sys.setenv(DATABASE_CONNECTOR_JAR_FOLDER = driver_path)
Sys.setenv(DATABASE_SERVER = 'localhost/ohdsi')
Sys.setenv(DATABASE_PWD = 'supersecret')

expected_wd = paste0(base_path, '/workdir/vignettes')
if (getwd() != expected_wd){
  	setwd(cat(getwd(), '/workdir/vignettes/'))  
}
```

_* Note: We hid our host and password fields with default values in order to block access to our MIMIC-III Database._

In case of requiring more memory use, use the following command to increase the memory to be used by the JDBC drivers:
```{r echo=F, results='hide'}
options(java.parameters = "-Xmx2000m")
```

Then, we can establish a connection with our MIMIC-III database as follows:
```{r echo=F, results='hide'}
cdm_database_schema = 'omop'
target_database_schema = 'omop'
target_cohort_table = 'atlas_cohorts'
database_name = 'mimic'

connectionDetails = DatabaseConnector::createConnectionDetails(
	  dbms = 'postgresql',  
	  user = 'postgres',
	  password = Sys.getenv('DATABASE_PWD'),
	  server = Sys.getenv('DATABASE_SERVER'),
	  port = 5432,
	  extraSettings = 'ssl=true;',
	  pathToDriver = Sys.getenv('DATABASE_CONNECTOR_JAR_FOLDER')
)

connection = DatabaseConnector::connect(connectionDetails)
DatabaseConnector::dbListTables(connection, schema = cdm_database_schema)
```

### Loading cohorts
Firstly, we need to [create the cohort table](https://gist.github.com/demarchenac/cddd3cab8f1938173042106a6323ebd1):
```{r echo=F, results='hide'}
dir.create(file.path(getwd(), 'atlas_cohorts'), showWarnings = FALSE)
download.file(
	  'https://gist.githubusercontent.com/demarchenac/cddd3cab8f1938173042106a6323ebd1/raw/96e774c2fa38ef7bdcada137cf57c636ed9670a2/reset_cohorts.sql',
	  file.path(getwd(), 'atlas_cohorts', 'reset_cohorts.sql')
)

sql = SqlRender::readSql(file.path(getwd(), 'atlas_cohorts', 'reset_cohorts.sql'))

sql = SqlRender::render(
	  sql, 
	  cdm_database_schema = cdm_database_schema, 
	  cohort_table = target_cohort_table
)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::executeSql(connection, sql)
```

Furthermore, we proceed as follows in order to load the [target cohort](https://gist.github.com/demarchenac/a13e3b4832febcfe2203159560d90106):
```{r echo=F, results='hide'}
download.file(
	  'https://gist.githubusercontent.com/demarchenac/a13e3b4832febcfe2203159560d90106/raw/03ec3f7854d495eb1e184487db13622bfd5cb6ef/target_cohort.sql', 
	  file.path(getwd(), 'atlas_cohorts', 'target_cohort.sql')
)

sql = SqlRender::readSql(file.path(getwd(), 'atlas_cohorts', 'target_cohort.sql'))

sql = SqlRender::render(
	  sql, 
	  cdm_database_schema = cdm_database_schema, 
	  target_cohort_table = target_cohort_table,
	  target_database_schema = target_database_schema,
	  vocabulary_database_schema = cdm_database_schema,
	  target_cohort_id = '4'
)

# Manually replace codesets
gs = gsub("#Codesets", paste0(cdm_database_schema, ".codesets"), sql)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::executeSql(connection, sql)
```

Then, we repeat this process with the [outcome cohort](https://gist.github.com/demarchenac/a6908b7a6b390a0dbc4d41265c9b03a7):
```{r echo=F, results='hide'}
download.file(
	  'https://gist.githubusercontent.com/demarchenac/a6908b7a6b390a0dbc4d41265c9b03a7/raw/5e6dfbea6d95faf744e6ad8d122c4807cf668562/outcome_cohort.sql', 
	  file.path(getwd(), 'atlas_cohorts', 'outcome_cohort.sql')
)

sql = SqlRender::readSql(file.path(getwd(), 'atlas_cohorts', 'outcome_cohort.sql'))

sql = SqlRender::render(
	  sql, 
	  cdm_database_schema = cdm_database_schema, 
	  target_cohort_table = target_cohort_table,
	  target_database_schema = target_database_schema,
	  target_cohort_id = '3'
)

# Manually replace codesets
gs = gsub("#Codesets", paste0(cdm_database_schema, ".codesets"), sql)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::executeSql(connection, sql)
```

In addition, we can check our cohort generation with the following R code:
```{r echo=F, results='hide'}
sql = paste(
	  'SELECT cohort_definition_id, COUNT(*) AS count',
	  'FROM @output_schema.@cohort_table',
	  'GROUP BY cohort_definition_id'
)

sql = SqlRender::render(
	  sql, 
	  output_schema = target_database_schema, 
	  cohort_table = target_cohort_table
)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::querySql(connection, sql)
```

And we can view some entries with the following R code:
```{r echo=F, results='hide'}
sql = paste('SELECT * FROM @output_schema.@cohort_table LIMIT 5')

sql = SqlRender::render(
	  sql, 
	  output_schema = target_database_schema, 
	  cohort_table = target_cohort_table
)

sql = SqlRender::translate(sql, targetDialect = connectionDetails$dbms)

DatabaseConnector::querySql(connection, sql)

DatabaseConnector::disconnect(connection)
```

### Study script creation
In this section, we will explain how to continue the R script that will execute our study as we have defined previously. Remember that our cohorts have been created by using [ATLAS](#atlas-cohort-builder).

### Data extraction
Other inclusion criteria can also be defined to extract the relevant characteristics, for which we use the createCovariateSettings method, which we can find more information in the [following link](https://ohdsi.github.io/FeatureExtraction/reference/createCovariateSettings.html).

```{r echo=F, results='hide'}
covariateSettings <- FeatureExtraction::createCovariateSettings(
  	useDemographicsGender = TRUE,
  	useDemographicsAge = TRUE,
  	useConditionGroupEraLongTerm = TRUE,
  	useConditionGroupEraAnyTimePrior = TRUE,
	useDrugGroupEraLongTerm = TRUE,
  	useDemographicsRace = TRUE,
	useConditionOccurrenceAnyTimePrior = TRUE,
	useConditionOccurrenceLongTerm = TRUE,
	useConditionOccurrenceMediumTerm = TRUE,
	useConditionOccurrenceShortTerm = TRUE,
	useDemographicsAgeGroup = TRUE,
	useConditionEraAnyTimePrior = TRUE,
	useProcedureOccurrenceAnyTimePrior = TRUE,
	useMeasurementAnyTimePrior = TRUE,
	useMeasurementValueAnyTimePrior = TRUE,
	useDrugGroupEraAnyTimePrior = TRUE,
	useCharlsonIndex = TRUE,
	useDcsi = TRUE,
	useChads2 = TRUE,
	useChads2Vasc = TRUE,
	useVisitConceptCountLongTerm = TRUE,
	longTermStartDays = -365,
	endDays = -1
)
```

Create a configuration that contains the details about the cdmDatabase connection for data extraction, for more information see [how to create database details](https://ohdsi.github.io/PatientLevelPrediction/reference/createDatabaseDetails.html).
```{r echo=F, results='hide'}
databaseDetails <- PatientLevelPrediction::createDatabaseDetails(
  	connectionDetails = connectionDetails,
	cdmDatabaseSchema = cdm_database_schema,
	cdmDatabaseName = database_name,
	cohortDatabaseSchema = target_database_schema,
  	cohortTable = target_cohort_table,
	cohortId = 4,
	outcomeDatabaseSchema = target_database_schema,
	outcomeTable = target_cohort_table,
	outcomeIds = 3,
	cdmVersion = 5
)
```

We restrict the sample size if we do not feel we have enough computing power or if we only want to run a small sample:
	
```{r echo=F, results='hide'}
restrictPlpDataSettings <- PatientLevelPrediction::createRestrictPlpDataSettings(sampleSize = 100000)
```

This function executes a large set of SQL statements against the database in OMOP CDM format to extract the data needed to perform the analysis, for more information see [how to obtain PLP data](https://rdrr.io/github/OHDSI/PatientLevelPrediction/man/getPlpData.html).
Once the necessary data has been extracted, we save it in case we want to replicate the example:

```{r echo=F, results='hide'}
dir.create(file.path(getwd(), 'atlas_plp_data'), showWarnings = FALSE)

plpData <- PatientLevelPrediction::getPlpData(
  	databaseDetails = databaseDetails,
	covariateSettings = covariateSettings,
	restrictPlpDataSettings = restrictPlpDataSettings
)
	
PatientLevelPrediction::savePlpData(plpData, file.path(getwd(), 'atlas_plp_data'))
```

### Additional inclusion criteria
The configuration for the study population is created, which details information such as the start and end of the cut, risk window and the outcome. For more information about the method you can read it's [documentation](https://www.rdocumentation.org/packages/PatientLevelPrediction/versions/4.3.10/topics/createStudyPopulationSettings). 
```{r echo=F, results='hide'}
populationSettings <- PatientLevelPrediction::createStudyPopulationSettings(
  	washoutPeriod = 0,
	firstExposureOnly = TRUE,
	removeSubjectsWithPriorOutcome = TRUE,
	priorOutcomeLookback = 99999,
	riskWindowStart = 1,
	riskWindowEnd = 90,
	startAnchor = 'cohort start',
  	endAnchor = 'cohort start',
	minTimeAtRisk = 30,
	requireTimeAtRisk = TRUE,
	includeAllOutcomes = TRUE
)
```

### Spliting the data into training/validation/testing datasets
Continuing with the steps for model training, we divide the data into training, validation and test sets, this with the values defined for cross-validation.
```{r echo=F, results='hide'}
splitSettings <- PatientLevelPrediction::createDefaultSplitSetting(
  	trainFraction = 0.75,
	testFraction = 0.25,
	type = 'stratified',
	nfold = 5, 
	splitSeed = 1234
)
```

### Preprocessing the training data
 
It was used the default sample settings. it simply returns the trainData as input, see below: 

```{r echo=F, results='hide'}
sampleSettings <- PatientLevelPrediction::createSampleSettings()
```

However, the current package contains methods of under-sampling the non-outcome patients.  To perform undersampling, the `type` input should be 'underSample' and `numberOutcomestoNonOutcomes` must be specified (an integer specifying the number of non-outcomes per outcome).  It is possible to add any custom function for over/under sampling, see [vignette for custom sampling](https://github.com/OHDSI/PatientLevelPrediction/blob/master/inst/doc/AddingCustomSamples.pdf).

It is possible to specify a combination of feature engineering functions that take as input the trainData and output a new trainData with different features.  The default feature engineering setting does nothing:

```{r echo=F, results='hide'}
featureEngineeringSettings <- PatientLevelPrediction::createFeatureEngineeringSettings()
```

Finally, the preprocessing settings.  For this setting it was defined `minFraction = 0.01`, this removes any features that is observed in the training data for less than 0.01 fraction of the patients. The input `normalize = T` specifies whether the features are scaled between 0 and 1.  The input `removeRedundancy = T` specifies whether features that are observed in all of the target population are removed.

```{r echo=F, results='hide'}
preprocessSettings <- PatientLevelPrediction::createPreprocessSettings(
	minFraction = 0.01, 
	normalize = T, 
	removeRedundancy = T
)
```

### Model development

For the algorithm selection with considered two options:

- [setLassoLogisticRegression](https://ohdsi.github.io/PatientLevelPrediction/reference/setLassoLogisticRegression.html) : [LASSO regression](https://en.wikipedia.org/wiki/Lasso_(statistics)) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.

- [setGradientBoostingMachine](https://ohdsi.github.io/PatientLevelPrediction/reference/setGradientBoostingMachine.html) : [Gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) is a machine learning technique used in regression and classification tasks, among others. It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees.

For our problem we choose to build a logistic regression model with the default hyper-parameters
```{r echo=F, results='hide'}
lrModel <- PatientLevelPrediction::setLassoLogisticRegression()
```

For the gradient boosting machines model it was used the hyper-parameters explained in [vignette for building patient-level predictive models](https://github.com/OHDSI/PatientLevelPrediction/blob/main/inst/doc/BuildingPredictiveModels.pdf).

```{r echo=F, results='hide'}
lrModel <- PatientLevelPrediction::setGradientBoostingMachine(
  	ntrees = 5000, 
  	maxDepth = c(4, 7, 10), 
  	learnRate = c(0.001, 0.01, 0.1, 0.9)
)
```

The `runPlP` function requires the `plpData`, the `outcomeId` specifying the outcome being predicted and the settings: `populationSettings`, `splitSettings`, `sampleSettings`, `featureEngineeringSettings`, `preprocessSettings` and `modelSettings` to train and evaluate the model. 

```{r echo=F, results='hide'}
dir.create(file.path(getwd(), 'atlas_plp_model'), showWarnings = FALSE)

lrResults <- PatientLevelPrediction::runPlp(
	plpData = plpData,
	outcomeId = 3, 
	analysisId = 'Test',
	analysisName = 'Demonstration of runPlp for training single PLP models',
	populationSettings = populationSettings, 
	splitSettings = splitSettings,
	sampleSettings = sampleSettings, 
	featureEngineeringSettings = featureEngineeringSettings, 
	preprocessSettings = preprocessSettings,
	modelSettings = lrModel,
	logSettings = PatientLevelPrediction::createLogSettings(), 
	executeSettings = PatientLevelPrediction::createExecuteSettings(
		runSplitData = T, 
		runSampleData = T, 
		runfeatureEngineering = T, 
		runPreprocessData = T, 
		runModelDevelopment = T, 
		runCovariateSummary = T
	), 
	saveDirectory = file.path(getwd(), 'atlas_plp_model')
)
```

### Result analysis
Running the following R code block we can review the results of a given model:

```{r echo=F, results='hide'}
library(PatientLevelPrediction)
library(dplyr)
viewPlp(lrResults)
```

As a starter, we get the following confunsion matrix for the Lasso model:

|                        | **Ground Truth Negative** | **Ground Truth Positive** |
|:-----------------------|--------------------------:|--------------------------:|
| **Predicted Positive** | 44                        | 357                       |
| **Predicted Negative** | 239                       | 319                       |


Furthermore, we also have the following confusion matrix for the Gradient Boost model:

|                        | **Ground Truth Negative** | **Ground Truth Positive** |
|:-----------------------|--------------------------:|--------------------------:|
| **Predicted Positive** | 54                        | 420                       |
| **Predicted Negative** | 229                       | 256                       |

In addition, we can compute the following metrics from each confusion matrix:

| **Metric**      | **Lasso Model Value** | **XGBoost Model Value** |
|:----------------|:---------------------:|:-----------------------:|
| **Threshold**   | 0.849                 | 0.805                   |
| **Specificity** | 84.5%                 | 80.9%                   |
| **Incidence**   | 70.49%                | 70.49%                  |
| **Sensitivity** | 52.8%                 | 62.1%                   |
| **PPV**         | 89%                   | 88.6%                   |
| **NPV**         | 42.8%                 | 47.2%                   |

As an initial observation, we notice that both models behave pretty simlar since we get little to no difference regarding the model performance.

We can notice that we get the same incidence on each model, which makes sense since this metric talks about the amount of pacients affected which in both models should remain the same. 

Also, we can also notice that the specificity and the sensitivity are related in a inversed ratio since we noticed how one variable directly affects the ratio from the other one. This makes sense since in this case the specificity talks about the pacient survival rate (avoinding the outcome when it really din't have the outcome) and the sensitivity talks about the death rate (Since it talks about presenting the outcome when it should).

### References
Johnson, A., Pollard, T., & Mark, R. (2016). MIMIC-III Clinical Database (version 1.4). PhysioNet. https://doi.org/10.13026/C2XW26.
